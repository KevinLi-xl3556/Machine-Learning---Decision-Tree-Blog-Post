{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f48494",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "I discuss some basic concepts and materials that I learnt in class and some supplementary information that I found on line. The source (citation) can be found in the readme file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970b7ab",
   "metadata": {},
   "source": [
    "### 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69124766",
   "metadata": {},
   "source": [
    "When I think of decision tree, the computer science notion \"divide and conquer\" appears in my mind. It can be applied in many problems (the simplest one being binary classification). Suppose I want to determine if I am able to take the machine learning class. I may construct a decision tree:\n",
    "        \n",
    "        Intro to CS ?\n",
    "       |            |\n",
    "       |NO          |Yes\n",
    "       |            \\  \n",
    "    Don't take         Data Structures ?\n",
    "                      \n",
    "                      |              |\n",
    "                      |No            |Yes\n",
    "                      /              \\\n",
    "            Don't take                Math Requirement (Linear Algebra & Probability & Stats)\n",
    "                                        |                           |\n",
    "                                        |No                         |Yes\n",
    "                                        /                           \\\n",
    "                                   Don't take                        Take\n",
    "                                  \n",
    "The goal of the decision tree is towrite the set of questions and guesses in a tree formate, in order to figure out \"what questions to ask, in what order to ask them, and what answer to predict once you have asked enough questions\". The decision tree is interpretable and intuitive, it models discrete outcomes nicely, and can be very powerful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659176f4",
   "metadata": {},
   "source": [
    "### 1. Materials and Basic Concepts I learnt in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef92bfe",
   "metadata": {},
   "source": [
    "##### 1.1 Confusion Matrix (error matrix)\n",
    "Confusion matrix is a table layout that allows visualization of the performance of an algorithm. In the example during class, our initial confusion matrix contain:\n",
    "\n",
    "   1. Condition Positive (P): The number of real positive cases in the data\n",
    "    \n",
    "   2. Condition Negative (N): The number of real negative cases in the data\n",
    "    \n",
    "   3. True Positive (TP, hit): A test result that correctly indicates the presence of a condition or characteristic. In our case, it's the person that is tested positive and truely gets covid.   \n",
    "    \n",
    "   4. False Negative (FN, type II error, miss): A test result which wrongly indicates that a particular condition or attribute is absent. In our case, it's the person that is tested negative but gets covid.    \n",
    "    \n",
    "   5. False Positive (FP, type I error, false alarm): A test result which wrongly indicates that a particular condition or attribute is present.In our case, it's the person that is tested positive but doesn't get covid.    \n",
    "    \n",
    "   6. True Negative (TN, correct rejection): A test result that correctly indicates the absence of a condition or characteristic. In our case, it's the person that are tested negative and doesn't get covid.\n",
    "\n",
    "After that, we introduce some more terminology to complement the Confusion Matrix, including (Prevalence, ACC, BA, F1, MCC, FM, BM, deltap, DOR is not included): \n",
    "\n",
    "   1. True Positive Rate ($TPR$): $\\frac{TP}{P}=1-FNR$\n",
    "    \n",
    "   2. True Negative Rate ($TNR$): $\\frac{TN}{N}=1-FPR$\n",
    "    \n",
    "   3. Positive Predictive Value ($PPV$): $\\frac{TP}{TP+FP}=1-FDR$\n",
    "    \n",
    "   4. Negative Predictive Value ($NPV$): $\\frac{TN}{TN+FN}=1-FOR$\n",
    "    \n",
    "   5. False Negative Rate ($FNR$): $\\frac{FN}{P}=1-TPR$\n",
    "    \n",
    "   6. False Positive Rate ($FPR$): $\\frac{FP}{N}=1-TNR$\n",
    "    \n",
    "   7. False Discovery Rate ($FDR$): $\\frac{FP}{FP+TP}=1-PPV$\n",
    "    \n",
    "   8. False Omission Rate ($FOR$): $\\frac{FN}{FN+TN}=1-NPR$\n",
    "    \n",
    "   9. Positive Likelihood Ratio ($LR+$): $\\frac{TPR}{FPR}$\n",
    "    \n",
    "   10. Negative Likelihood Ratio ($LR-$): $\\frac{FNR}{TNR}$\n",
    "    \n",
    "   11. Prevalence Threshold ($PT$): $\\frac{\\sqrt(FPR)}{\\sqrt(TPR)+\\sqrt(FPR)}$\n",
    "    \n",
    "   12. Threat Score ($TS$): $\\frac{TP}{TP+FN+FP}$ \n",
    "    \n",
    "To sum up, Confusion matrix is a special kind of table with two dimensions including \"Actual condition\" and \"Predicted condition\", and relevant info in both dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b25db",
   "metadata": {},
   "source": [
    "##### 1.2 ROC curve\n",
    "Receiver Operating Characteristic (ROC) curve is a garphical plot that illustrates the diagnostic ability of a binary classfier, It plots the True Positive Rate against the False Positive Rate. The best possible prediction method would yield points in the upper left corner or coordinate (0,1). This represents no false negatives and no false positives. The point (0,1) is called perfect classfication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f48199",
   "metadata": {},
   "source": [
    "##### 1.3 Decision Tree\n",
    "###### 1.3.1 Hypotheses: Decision Trees f:X->Y (from  Additional Reading)\n",
    "    -Each internal node tests an attribute $x_i$\n",
    "\n",
    "    -One branch for each possible attribute value $x_i=v$\n",
    "\n",
    "    -Each leaf assigns a class y\n",
    "\n",
    "    -To classify input x: traverse the tree from root to leaf, out put the labeled y\n",
    "    \n",
    "###### 1.3.2 What functions can be represented?\n",
    "    -Can represent any function of the input attributes\n",
    "    \n",
    "    -Could require exponentially many nodes\n",
    "    \n",
    "###### 1.3.3 Simplest Decision Tree\n",
    "    - Learning the simplest decision tree is an NP-complete problem\n",
    "    \n",
    "    - Use Greedy Recursive Heuristic:\n",
    "    \n",
    "        -Start from empty decision tree\n",
    "        \n",
    "        -Split on next best attribute (feature)\n",
    "        \n",
    "        -Recurse\n",
    "        \n",
    "###### 1.3.4 Uncertainty\n",
    "\n",
    "In the greedy heuristic, we need to find a good attribute, which means that we need to find a good split. A split is a good split if we are more certain about classification after split. For example, a uniform distribution is bad. To measure the uncertainty, we introduce the idea entropy. The entropy $H(Y)$ of a random variable Y is: \n",
    "\n",
    "$$H(Y)=-\\sum_{i=1}^kP(Y=y_i)log_2P(Y=y_i)$$\n",
    "\n",
    "In general, more uncertainty means more entropy. The idea is illustrated as:\n",
    "    \n",
    "    High Entropy\n",
    "        \n",
    "        -Y is from a uniform like distribution\n",
    "        \n",
    "        -Flat histogram\n",
    "        \n",
    "        -Less predictable\n",
    "    \n",
    "    Low Entropy\n",
    "    \n",
    "        -Y is from a varied distribution, often has peaks and valleys\n",
    "        \n",
    "        -Histogram is not flat\n",
    "        \n",
    "        -Values sampled from it is more predictable\n",
    "        \n",
    "We introduce other terminologies: \n",
    "\n",
    "Conditinal Entropy $H(Y|X)$ of Y conditioned on X:\n",
    "\n",
    "$$H(Y|X)=-\\sum_{j=1}^v P(X=x_j)\\sum_{i=1}^k P(Y=y_i|X=x_j)log_2P(Y=y_i | X=x_j)$$\n",
    "\n",
    "Information Gain:\n",
    "\n",
    "$$IG(X) = H(Y)-H(Y|X)$$\n",
    "\n",
    "measures the decrease in entropy after splitting. When IG is greater than 0, it means that we prefer the split.\n",
    "\n",
    "###### 1.3.5 Learning for Decision Trees\n",
    "Now our algorithm to construct simplest decision tree becomes:\n",
    "\n",
    "   1. Start from empty decision tree\n",
    "    \n",
    "   2. Use Information Gain to select attribute: $arg max_i IG(X_i)=arg max_i H(Y)-H(Y|X_i)$\n",
    "   \n",
    "   3. Recurse\n",
    "   \n",
    "When to stop splitting:\n",
    "\n",
    "    -if all matching records have the same output value then don't recurse\n",
    "    \n",
    "    -if all records have exactly the same set of input attributes then don't recurse\n",
    "    \n",
    "###### 1.3.6 Decision Tree Overfit\n",
    "Decision trees will overfit. We must find a low bias with low variance decision tree, we can do this by using random forest, which outputs the class selected by most trees. Random forest is the next lecture, and will not be in this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7f4a3",
   "metadata": {},
   "source": [
    "### 2. Some Supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203608a",
   "metadata": {},
   "source": [
    "##### 2.1 Combating Overfitting in Decision Trees (Other than Random Forests)\n",
    "In my mind, there should be a simpler way to reduce the overfitting. Here is what I find online:\n",
    "\n",
    "    -We can stop splitting leaves past a fixed depth \n",
    "    \n",
    "    -We can stop splitting leaves with fewer than a fixed number of data points\n",
    "    \n",
    "    -We can stop splitting leaves when the maximal information gain is less than a fixed number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2672c9",
   "metadata": {},
   "source": [
    "##### 2.2 Pros and Cons\n",
    "    Pros\n",
    "        \n",
    "        -Interpretable\n",
    "        \n",
    "        -Efficient (when studying the computational cost and storage)\n",
    "        \n",
    "        -Useful when there is classification and regression task\n",
    "        \n",
    "        -Compatible with categorical and real-valued features\n",
    "        \n",
    "    Cons\n",
    "    \n",
    "        -Each split only considers the immediate impact on the splitting criterion, it doesn't guarantee to find the smallest tree\n",
    "        \n",
    "        -Overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
